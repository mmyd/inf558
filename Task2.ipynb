{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Snorkel to Extract Educational Background of Artists\n",
    "\n",
    "**Note**: \n",
    "- Much content of this notebook was borrowed from Snorkel Introduction Tutorial\n",
    "- You are supposed to write your code or modify our code in any cell starting with `# ** STUDENT CODE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art extraction techniques require massive labeled training set but it is costly to obtain. To overcome this problem, Snorkel helps rapidly create training sets using the new data programming paradigm. To start, developers focus on writing a set of labeling functions, which are just scripts that programmatically label data. The resulting labels are noisy, but Snorkel uses a generative model to learn how to use those labeling functions to label more data. The new labeled data now can be used to train high-quality end models.\n",
    "\n",
    "In summary, in this task, you will first manually label 50 documents and use these labeled data as a development set to create your own labeling functions. Then, you will train a generative model to label 800 documents in training set. Finally, you will train Bi-LSTM to extract final extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing and creating dev set**\n",
    "\n",
    "Before you proceed with task 2.1, we need to preprocess our documents using `Snorkel` utilities, parsing them into a simple hierarchy of component parts of our input data, which we refer as _contexts_. We'll also create _candidates_ out of these contexts, which are the objects we want to classify, in this case, possible mentions of schools. Finally, we'll load some gold labels for evaluation.\n",
    "\n",
    "All of this preprocessed input data is saved to a database. In Snorkel, if no database is specified, then a SQLite database at `./snorkel.db` is created by default -- so no setup is needed here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from snorkel.lf_helpers import get_left_tokens, get_right_tokens, get_between_tokens\n",
    "\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TSVDocPreprocessor, CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from snorkel.annotations import LabelAnnotator, load_gold_labels\n",
    "\n",
    "from utils import reload_external_labels, save_gold_labels, save_predicted_relations, save_gold_relations, get_dev_doc_ids, get_test_doc_ids, get_gold_labels, number_of_people\n",
    "\n",
    "# TODO: Set location where you store your homework 2\n",
    "if 'HW_DIR' not in os.environ:\n",
    "    HW_DIR = Path(\"/Volumes/E/HJH/USC/DTIN/INF558/18fall/Homeworks/Homework2\")\n",
    "else:\n",
    "    HW_DIR = Path(os.environ['HW_DIR'])\n",
    "    assert HW_DIR.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing a `SnorkelSession`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Corpus**\n",
    "\n",
    "Next, we load and pre-process the corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_preprocessor = TSVDocPreprocessor(HW_DIR / 'artists_bio.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running a `CorpusParser`**\n",
    "\n",
    "We'll use [Spacy](https://spacy.io/), an NLP preprocessing tool, to split our documents into sentences and tokens, and provide named entity annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 24.9 s, sys: 406 ms, total: 25.3 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use simple database queries (written in the syntax of [SQLAlchemy](http://www.sqlalchemy.org/), which Snorkel uses) to check how many documents and sentences were parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 1105\n",
      "Sentences: 10569\n"
     ]
    }
   ],
   "source": [
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Candidates**\n",
    "\n",
    "The next step is to extract _candidates_ from our corpus. A `Candidate` in Snorkel is an object for which we want to make a prediction. In this case, the candidates are pairs of person and organization mentioned in sentences.\n",
    "\n",
    "The [Spacy](https://spacy.io/) parser we used performs _named entity recognition_ for us. Next, we'll split up the documents into train, development, and test splits; and collect the associated sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Education = candidate_subclass('Education', ['person', 'organization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher, OrganizationMatcher\n",
    "\n",
    "ngrams         = Ngrams(n_max=7)\n",
    "person_matcher = PersonMatcher(longest_match_only=True)\n",
    "organization_matcher = OrganizationMatcher(longest_match_only=True)\n",
    "cand_extractor = CandidateExtractor(Education, [ngrams, ngrams], [person_matcher, organization_matcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dev sents: 541\n",
      "Number of train sents: 7025\n",
      "Number of test sents: 2953\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "dev_docs = get_dev_doc_ids(HW_DIR / \"artists.dev.txt\")\n",
    "test_docs = get_test_doc_ids(HW_DIR / \"artists.test.txt\")\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for doc in docs:\n",
    "    sents = (s for s in doc.sentences if number_of_people(s) <= 5)\n",
    "    if doc.name in dev_docs:\n",
    "        dev_sents.update(sents)\n",
    "    elif doc.name in test_docs:\n",
    "        test_sents.update(sents)\n",
    "    else:\n",
    "        train_sents.update(sents)\n",
    "        \n",
    "print(\"Number of dev sents:\", len(dev_sents))\n",
    "print(\"Number of train sents:\", len(train_sents))\n",
    "print(\"Number of test sents:\", len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll apply the candidate extractor to the three sets of sentences. The results will be persisted in the database backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/7025 [00:00<00:34, 202.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7025/7025 [00:38<00:00, 181.04it/s]\n",
      "  3%|▎         | 18/541 [00:00<00:02, 175.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 4415\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541/541 [00:03<00:00, 157.23it/s]\n",
      "  1%|          | 20/2953 [00:00<00:16, 175.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 285\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2953/2953 [00:16<00:00, 182.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 1560\n",
      "CPU times: user 57.6 s, sys: 752 ms, total: 58.4 s\n",
      "Wall time: 58.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    cand_extractor.apply(sents, split=i)\n",
    "    print(\"Number of candidates:\", session.query(Education).filter(Education.split == i).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1. Label 50 documents in development set\n",
    "\n",
    "In this task, you will use `SentenceNgramViewer` to label each mention. You can click Blue button to mark the candidate as correct, Red button to mark as incorrect. Your labeling result is automatically stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number unlabeled: 285\n"
     ]
    }
   ],
   "source": [
    "gold_labels = get_gold_labels(session)\n",
    "labeled_sents = {lbl.candidate.person.sentence.id for lbl in gold_labels}\n",
    "unlabeled = [\n",
    "    x for x in session.query(Education).filter(Education.split == 1).all() \n",
    "    if x.person.sentence.id not in labeled_sents\n",
    "]\n",
    "print(\"Number unlabeled:\", len(unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SentenceNgramViewer(cids=[[[201], [154, 155, 222, 223, 244, 245], [114, 211]], [[80], [137], [50, 51, 127, 128…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SentenceNgramViewer(unlabeled, session, annotator_name=\"gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you finish labeling, executing the cell below to **save your result** to JSON files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_gold_labels(session, HW_DIR / \"gold_labels.dev.json\", split=1)\n",
    "save_gold_relations(session, HW_DIR / \"extracted_relation.dev.json\", split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 Define labeling functions (LFs)\n",
    "\n",
    "In this task, you will define your own LFs, which Snorkel uses to create noise-aware training set. Usually, you will go through a couple of iterations (create LFs, test and refine it) to come up with a good set of LFs. We provide you at the end of Task 2.2 a helper to quickly see what candidates did your model fail to classify. You can refer to Snorkel tutorial or online documentation for more information.\n",
    "\n",
    "You are free to use write any extra code to create a set of sophisticated LFs. For example, you build a list of universities and check if it matches with your candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_between = {'studied','study','began','begin','enrolled','entered','enter','enroll'}\n",
    "prep = {'at','to'}\n",
    "with_word = {'with'}\n",
    "from_word = {'from'}\n",
    "pounc_word = {','}\n",
    "get_words = {'received','receive','earned','earn'} \n",
    "non_words = {'work','worked','professor','taught','teach','teaching'}\n",
    "verb_before = {'studying','training','enrolling','begining'}\n",
    "school_words = {'Institute','College','University','Students League','Academy','School'}\n",
    "schools=['Institute','College','University','Students League','Academy','School']\n",
    "def school_determine(s):\n",
    "    for x in schools:\n",
    "        if s.find(x)+1:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def LF_between_words(c):\n",
    "    if school_determine(c.organization.get_span()):\n",
    "        if len(verb_between.intersection(get_between_tokens(c))) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def LF_at_with(c):\n",
    "    if school_determine(c.organization.get_span()):\n",
    "#         with sb at school / at school with sb\n",
    "        if len(prep.intersection(get_left_tokens(c.organization, window=2))) > 0 and len(with_word.intersection(get_left_tokens(c.person, window=2)))>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def LF_at_person(c):\n",
    "    if school_determine(c.organization.get_span()):\n",
    "#         studying at school, sb\n",
    "        if len(verb_before.intersection(get_left_tokens(c.organization, window=2))) > 0 and len(pounc_word.intersection(get_left_tokens(c.person, window=2))) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def LF_xxx_from(c):\n",
    "    if school_determine(c.organization.get_span()):\n",
    "#         earn/recieve from school\n",
    "        if len(from_word.intersection(get_between_tokens(c))) > 0 and len(get_words.intersection(get_between_tokens(c))) > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def LF_exclude(c):\n",
    "    if school_determine(c.organization.get_span()):\n",
    "        if len(non_words.intersection(get_between_tokens(c))) > 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFs = [LF_between_words,LF_at_with,LF_at_person,LF_xxx_from]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train generative model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/4415 [00:00<00:21, 207.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4415/4415 [00:19<00:00, 231.35it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1702)\n",
    "\n",
    "labeler = LabelAnnotator(lfs=LFs)\n",
    "L_train = labeler.apply(split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n",
      "LF weights: [ 1.60813749  1.54326976  1.50578612  1.52338638]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)\n",
    "\n",
    "print(\"LF weights:\", gen_model.weights.lf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the distribution of the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have learned the generative model, we will measure its performances using the provided test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 0\n",
      "AnnotatorLabels created: 0\n"
     ]
    }
   ],
   "source": [
    "# Load testset first\n",
    "reload_external_labels(session, HW_DIR / \"gold_labels.test.json\")\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 20/285 [00:00<00:01, 194.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [00:01<00:00, 187.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.677\n",
      "Neg. class accuracy: 0.992\n",
      "Precision            0.913\n",
      "Recall               0.677\n",
      "F1                   0.778\n",
      "----------------------------------------\n",
      "TP: 21 | FP: 2 | TN: 252 | FN: 10\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tp, fp, tn, fn = gen_model.error_analysis(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get detailed statistics of LFs learned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_between_words</th>\n",
       "      <td>0</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>227</td>\n",
       "      <td>0.991770</td>\n",
       "      <td>0.959030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_at_with</th>\n",
       "      <td>1</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>227</td>\n",
       "      <td>0.991489</td>\n",
       "      <td>0.959522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_at_person</th>\n",
       "      <td>2</td>\n",
       "      <td>0.803509</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>227</td>\n",
       "      <td>0.995633</td>\n",
       "      <td>0.951925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_xxx_from</th>\n",
       "      <td>3</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>227</td>\n",
       "      <td>0.995671</td>\n",
       "      <td>0.959395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  j  Coverage  Overlaps  Conflicts  TP  FP  FN   TN  \\\n",
       "LF_between_words  0  0.852632  0.810526        0.0  14   1   1  227   \n",
       "LF_at_with        1  0.824561  0.810526        0.0   6   1   1  227   \n",
       "LF_at_person      2  0.803509  0.800000        0.0   1   0   1  227   \n",
       "LF_xxx_from       3  0.810526  0.800000        0.0   3   0   1  227   \n",
       "\n",
       "                  Empirical Acc.  Learned Acc.  \n",
       "LF_between_words        0.991770      0.959030  \n",
       "LF_at_with              0.991489      0.959522  \n",
       "LF_at_person            0.995633      0.951925  \n",
       "LF_xxx_from             0.995671      0.959395  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to look at some examples in one of the error buckets to improve your LFs. For example, below is one of the false negatives that we did not correctly label as true mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SentenceNgramViewer(cids=[[[6], [7, 8, 9], [3, 4]], [[2], [0], [5]], [[1]]], html='<head>\\n<style>\\nspan.candi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SentenceNgramViewer(fn, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3. Training an End Extraction Model\n",
    "\n",
    "In this final task, we'll use the noisy training labels we generated to train our end extraction model. In particular, we will be training a Bi-LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = session.query(Education).filter(Education.split == 0).order_by(Education.id).all()\n",
    "dev_cands   = session.query(Education).filter(Education.split == 1).order_by(Education.id).all()\n",
    "test_cands  = session.query(Education).filter(Education.split == 2).order_by(Education.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try tuning the hyper-parameters below to get your best F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Training model\n",
      "[LSTM] n_train=3885  #epochs=30  batch size=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianhenghou/anaconda2/envs/snorkel/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Epoch 1 (7.43s)\tAverage loss=0.354183\tDev F1=50.00\n",
      "[LSTM] Epoch 2 (15.32s)\tAverage loss=0.194354\tDev F1=65.82\n",
      "[LSTM] Epoch 3 (23.23s)\tAverage loss=0.150556\tDev F1=72.29\n",
      "[LSTM] Epoch 4 (31.30s)\tAverage loss=0.126924\tDev F1=68.57\n",
      "[LSTM] Epoch 5 (39.24s)\tAverage loss=0.119673\tDev F1=66.67\n",
      "[LSTM] Epoch 6 (47.29s)\tAverage loss=0.114983\tDev F1=65.79\n",
      "[LSTM] Epoch 7 (55.26s)\tAverage loss=0.111313\tDev F1=64.86\n",
      "[LSTM] Epoch 8 (63.16s)\tAverage loss=0.114895\tDev F1=65.12\n",
      "[LSTM] Epoch 9 (71.10s)\tAverage loss=0.146635\tDev F1=57.53\n",
      "[LSTM] Epoch 10 (79.01s)\tAverage loss=0.124181\tDev F1=65.75\n",
      "[LSTM] Epoch 11 (86.97s)\tAverage loss=0.114815\tDev F1=66.67\n",
      "[LSTM] Epoch 12 (94.84s)\tAverage loss=0.112341\tDev F1=68.35\n",
      "[LSTM] Epoch 13 (102.69s)\tAverage loss=0.110940\tDev F1=65.82\n",
      "[LSTM] Epoch 14 (110.66s)\tAverage loss=0.109253\tDev F1=70.59\n",
      "[LSTM] Epoch 15 (118.60s)\tAverage loss=0.106958\tDev F1=61.33\n",
      "[LSTM] Epoch 16 (126.56s)\tAverage loss=0.105473\tDev F1=62.86\n",
      "[LSTM] Epoch 17 (134.42s)\tAverage loss=0.106023\tDev F1=68.66\n",
      "[LSTM] Epoch 18 (142.28s)\tAverage loss=0.107834\tDev F1=62.16\n",
      "[LSTM] Epoch 19 (150.51s)\tAverage loss=0.105008\tDev F1=59.74\n",
      "[LSTM] Epoch 20 (158.55s)\tAverage loss=0.103544\tDev F1=63.89\n",
      "[LSTM] Epoch 21 (166.37s)\tAverage loss=0.103812\tDev F1=62.86\n",
      "[LSTM] Epoch 22 (174.19s)\tAverage loss=0.102632\tDev F1=65.00\n",
      "[LSTM] Epoch 23 (181.97s)\tAverage loss=0.102405\tDev F1=64.10\n",
      "[LSTM] Epoch 24 (190.02s)\tAverage loss=0.102415\tDev F1=64.00\n",
      "[LSTM] Model saved as <LSTM>\n",
      "[LSTM] Epoch 25 (198.14s)\tAverage loss=0.101597\tDev F1=73.02\n",
      "[LSTM] Model saved as <LSTM>\n",
      "[LSTM] Epoch 26 (206.17s)\tAverage loss=0.101103\tDev F1=66.67\n",
      "[LSTM] Epoch 27 (213.78s)\tAverage loss=0.100752\tDev F1=65.00\n",
      "[LSTM] Epoch 28 (221.29s)\tAverage loss=0.104086\tDev F1=61.73\n",
      "[LSTM] Epoch 29 (229.23s)\tAverage loss=0.100778\tDev F1=62.50\n",
      "[LSTM] Epoch 30 (237.51s)\tAverage loss=0.100573\tDev F1=66.67\n",
      "[LSTM] Training done (237.80s)\n",
      "[LSTM] Loaded model <LSTM>\n"
     ]
    }
   ],
   "source": [
    "# ** STUDENT CODE\n",
    "\n",
    "from snorkel.learning.pytorch import LSTM\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':            0.01,\n",
    "    'embedding_dim': 50,\n",
    "    'hidden_dim':    50,\n",
    "    'n_epochs':      30,\n",
    "    'dropout':       0.3,\n",
    "    'seed':          1701\n",
    "}\n",
    "\n",
    "lstm = LSTM(n_threads=None)\n",
    "lstm.train(train_cands, train_marginals, X_dev=dev_cands, Y_dev=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report performance of your final extractor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec: 0.270, Recall: 0.680, F1 Score: 0.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianhenghou/anaconda2/envs/snorkel/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "p, r, f1 = lstm.score(test_cands, L_gold_test)\n",
    "print(\"Prec: {0:.3f}, Recall: {1:.3f}, F1 Score: {2:.3f}\".format(p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.68\n",
      "Neg. class accuracy: 0.907\n",
      "Precision            0.27\n",
      "Recall               0.68\n",
      "F1                   0.386\n",
      "----------------------------------------\n",
      "TP: 51 | FP: 138 | TN: 1347 | FN: 24\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianhenghou/anaconda2/envs/snorkel/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "tp, fp, tn, fn = lstm.error_analysis(session, test_cands, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your new model to extract relation in testing documents, and save it to JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianhenghou/anaconda2/envs/snorkel/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "save_predicted_relations(HW_DIR / \"extracted_relation.test.json\", test_cands, lstm.predictions(test_cands))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
